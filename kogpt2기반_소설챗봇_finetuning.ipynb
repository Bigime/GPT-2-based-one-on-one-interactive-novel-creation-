{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHPBZoU_hQJn"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_lightning import Trainer,LightningModule\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "fpkF4N9eieyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='Simsimi based on KoGPT-2')\n",
        "\n",
        "parser.add_argument('--chat',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "\n",
        "parser.add_argument('--sentiment',\n",
        "                    type=str,\n",
        "                    default='0',\n",
        "                    help='sentiment for system. 0 is neutral, 1 is negative, 2 is positive.')\n",
        "\n",
        "parser.add_argument('--model_params',\n",
        "                    type=str,\n",
        "                    default='model_chp/model_-last.ckpt',\n",
        "                    help='model binary for starting chat')\n",
        "\n",
        "parser.add_argument('--train',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='for training')"
      ],
      "metadata": {
        "id": "2H3aG4Mrie0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "Q_TKN = \"<usr>\"\n",
        "A_TKN = \"<sys>\"\n",
        "BOS = \"</s>\"\n",
        "EOS = \"</s>\"\n",
        "MASK = \"<unused0>\"\n",
        "SENT = \"<unused1>\"\n",
        "PAD = \"<pad>\"\n",
        "\n",
        "\n",
        "TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\",\n",
        "    bos_token=BOS,\n",
        "    eos_token=EOS,\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=PAD,\n",
        "    mask_token=MASK,\n",
        "    additional_special_tokens=[\n",
        "        Q_TKN,\n",
        "        A_TKN,\n",
        "        SENT\n",
        "    ]\n",
        ")\n",
        "#사용자 지정 토큰을 만들면 additional_special_tokens에 추가해야된다.\n",
        "\n",
        "\n",
        "# 하나의 토큰으로 처리할 단어들\n",
        "new_tokens = ['결말', '도입', '발단', '위기', '전개',  '절정','경멸', '관심',  '두려움',  '미움', '분노', '슬픔',\n",
        "        '중립', '행복']\n",
        "\n",
        "# 토크나이저에 새로운 토큰 추가\n",
        "TOKENIZER.add_tokens(new_tokens)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
        "\n",
        "model.resize_token_embeddings(len(TOKENIZER))"
      ],
      "metadata": {
        "id": "IKi-UH_Nie28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('map_story.csv');df"
      ],
      "metadata": {
        "id": "iSRC6TB3ie5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['scene_content2']= df['scene_content'].shift(-1);df"
      ],
      "metadata": {
        "id": "_aq0orI0ie7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df.dropna(subset=['scene_content2']);df_1\n",
        "df_1 = df_1.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)\n",
        "df_1"
      ],
      "metadata": {
        "id": "-nzNM0duie9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = df_1.rename(columns={'scene_content':'Q','scene_content2':'A','narrative_stage' : 'stage','mapped_emotion':'emotion' });df_2"
      ],
      "metadata": {
        "id": "KlavskqMie_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = df_2.iloc[:270000,:]\n",
        "df_3"
      ],
      "metadata": {
        "id": "VS_tpwZHifDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, chats, max_len=32):\n",
        "        self._data = chats\n",
        "        self.first = True\n",
        "        self.q_token = Q_TKN\n",
        "        self.a_token = A_TKN\n",
        "        self.sent_token = SENT\n",
        "        self.bos = BOS\n",
        "        self.eos = EOS\n",
        "        self.mask = MASK\n",
        "        self.pad = PAD\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = TOKENIZER\n",
        "       # 위에서 tokenizer 적용 과정에서 썼던 token들 지정\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        turn = self._data.iloc[idx]\n",
        "        q = str(turn['Q'])\n",
        "        a = str(turn['A'])\n",
        "        stage=  str(turn['stage'])\n",
        "        emotion = str(turn['emotion'])\n",
        "       # q_toked = self.tokenizer.tokenize(self.bos + stage + self.sent_token + emotion + self.sent_token + self.q_token + q +\n",
        "       #                                   self.sent_token)\n",
        "        q_toked = self.tokenizer.tokenize(self.q_token + q )\n",
        "      # 이전 대사와 다음 대사만 가지고 일단 실험할 것이기 때문에 대사에 관련된 토큰만 집어넣음\n",
        "        q_len = len(q_toked)\n",
        "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
        "        a_len = len(a_toked)\n",
        "        if q_len + a_len > self.max_len:\n",
        "            a_len = self.max_len - q_len\n",
        "            if a_len <= 0:\n",
        "                q_toked = q_toked[-(int(self.max_len/2)):]\n",
        "                q_len = len(q_toked)\n",
        "                a_len = self.max_len - q_len\n",
        "                assert a_len > 0\n",
        "            a_toked = a_toked[:a_len]\n",
        "            a_len = len(a_toked)\n",
        "            assert a_len == len(a_toked), f'{a_len} ==? {len(a_toked)}'\n",
        "        # labels = [mask, mask, ...., mask,..,<usr>, A,.. <eos>]\n",
        "        labels = [\n",
        "            self.mask,\n",
        "        ] * q_len + a_toked[:]\n",
        "        if self.first:\n",
        "            logging.info(\"contexts : {}\".format(q))\n",
        "            logging.info(\"toked ctx: {}\".format(q_toked))\n",
        "            logging.info(\"response : {}\".format(a))\n",
        "            logging.info(\"toked response : {}\".format(a_toked))\n",
        "            logging.info('labels {}'.format(labels))\n",
        "            self.first = False\n",
        "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
        "        self.max_len\n",
        "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
        "        while len(labels_ids) < self.max_len:\n",
        "            labels_ids += [self.tokenizer.pad_token_id]\n",
        "        #labeling된 데이터에 padding 추가하여 max_length만큼 길이 조정\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
        "        while len(token_ids) < self.max_len:\n",
        "            token_ids += [self.tokenizer.pad_token_id]\n",
        "        #input으로 들어갈 토큰들의 임베딩 행렬에 padding추가하여 max_length만큼 길이 조정\n",
        "        return(token_ids, np.array(mask),\n",
        "               labels_ids)"
      ],
      "metadata": {
        "id": "sJFWaknZiya2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    data = np.array([np.array(item[0]) for item in batch])  # numpy 배열로 변환\n",
        "    mask = np.array([np.array(item[1]) for item in batch])\n",
        "    label = np.array([np.array(item[2]) for item in batch])\n",
        "\n",
        "\n",
        "    return (torch.LongTensor(data), torch.LongTensor(mask),\n",
        "            torch.LongTensor(label))\n"
      ],
      "metadata": {
        "id": "DFLX2Ridiyf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "H9yCP9d4iyiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_set = CharDataset(df_3, max_len=32)\n",
        "train_dataloader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=16,\n",
        "    num_workers=0,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "learning_rate = 5e-6\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epoch = 3\n",
        "Sneg = -1e18\n",
        "\n",
        "# 20% 단위로 진행 상태를 출력하는 기능 추가\n",
        "for epoch_num in range(epoch):\n",
        "    dataloader = tqdm(train_dataloader, desc=f\"Epoch {epoch_num}\")\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    total_steps = len(dataloader)\n",
        "    step_20_percent = total_steps // 5  # 전체 스텝의 20%에 해당하는 스텝 수 계산\n",
        "\n",
        "    for batch_idx, samples in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids, mask, label = samples\n",
        "        token_ids, mask, label = (token_ids.to(device),\n",
        "                                  mask.to(device),\n",
        "                                  label.to(device))\n",
        "\n",
        "        # 모델에 token_ids 전달\n",
        "        out = model(token_ids)\n",
        "        out = out.logits\n",
        "\n",
        "        # Mask 적용\n",
        "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
        "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
        "\n",
        "        # Loss 계산 및 역전파\n",
        "        loss = criterion(mask_out.transpose(2, 1), label)\n",
        "        avg_loss = loss.sum() / mask.sum()\n",
        "        avg_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += avg_loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "        # 20%, 40%, 60%, 80%, 100% 완료 시점에서 Average Loss 출력\n",
        "        if (batch_idx + 1) % step_20_percent == 0 or (batch_idx + 1) == total_steps:\n",
        "            percent_complete = (batch_idx + 1) / total_steps * 100\n",
        "            partial_loss = total_loss / total_batches\n",
        "            print(f\"Epoch {epoch_num} - {percent_complete:.0f}% Complete - Average Loss: {partial_loss:.4f}\")\n",
        "\n",
        "    # Epoch 끝에서 최종 Loss 출력\n",
        "    epoch_loss = total_loss / total_batches\n",
        "    print(f\"Epoch {epoch_num} - Final Average Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "-w_3abDQiylg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}